# Variant analysis recap:

*Performance Measurement* is essential for the business analysis, but we cannot rely on the median or average values, so we focus ourself on the distribution of variants, usually divided in 3 parts:

1. A few variants that describe a lot of cases
2. A good number of variants that describe a good number of cases
3. A lot of variants for a few cases

We focus ourself on the **part 1** or, maybe, on the **part 2**, but the **part 3** is near the useless.

The variant analysis is done on **event log** and we have different phases:

![image-20240325085837119](C:\Users\pietro\AppData\Roaming\Typora\typora-user-images\image-20240325085837119.png)

An event logs may contain **wrong or inaccurate information**. We consider it noise to be filtered out to improve our understanding of the process. 

There can be *errors in recording the events/cases* with activities no more relevant, with null time duration or with wrong timestamps.

And there can be recording of events/cases that are *incomplete*, even at attributes level (missing of timestamp, resource, cost...).

We can also *filtering irrilevant data*, so data that are not of interest for our analysis. For example we can divide the event logs in period of times because we are not interested on a specific time period :smile:.

We can even *filter for summarisation* to exclude information that we are not interested in. By simplifying our view on the process to improve our understanding we can relax the notion of varint speciying only:

- Start/End of activity
- Inclusion of specific activities (or grouping them)
- Cases with cycle-time between a range (so without including the real time)

We can also summarize for the *most frequent **activities** or **path***. 

> For example, focus on activities that represent the 90% of the total! We can learn a model from that

## Python:

```python
# importazione dati:
dF = pd.read_csv('https://raw.githubusercontent.com/paoloceravolo/BIS2022/main/Event%20Logs/CallCenterLog.csv',sep=',')
dF = pm4py.format_dataframe(dF, case_id='Case ID', activity_key='Activity', start_timestamp_key='Start Date', timestamp_key='End Date')

# Identificare le attivitá non frequenti:
dF.groupby('concept:name').size()

# Ora concentrati su quelle nulle:
dF.isnull().sum()

# Conversione in event_log e ottenimento varianti
event_log = pm4py.convert_to_event_log(dF)
variants = pm4py.get_variants(event_log)
# puoi anche usare la get_variants sul data frame direttamente
variants
plt.hist(variants)
plt.show()
```

> **Variants**:
>
> Identificano i vari passaggi tra le attivitá e il loro numero! Forse é ok togliere un certo numero di variants.

```python
# filtro tempi negativi
compute_case_duration(dF)
no_zero_duration = dF[dF['Duration'] > pd.Timedelta(0)]
no_zero_duration
```

> Fase di filtraggio finita dopo l'esclusione di certe attivitá di inizio. Vedi in vscode che hai fatto.
>
> Nella fase di filtraggio sarebbe anche bello individuare problemi di registrazione dei log, tipo quelli di durata negativa. Ma la loro registrazione errata dipende solo dal contesto.

```python
# ennesima prova dei valori medi, che sono delle merde
case_dur = filtered_log.groupby('Case ID')['Duration'].sum()
print('Mean duration', case_dur.mean())
print('Median duration', case_dur.median())
print('Mode duration', case_dur.mode().values[0])
print('STDev duration', case_dur.std())

import matplotlib.pyplot as plt
# va convertito...
case_dur = case_dur.dt.total_seconds().astype(int)
plt.hist(case_dur, bins=20, alpha=0.7)
plt.xlabel('Duration Mins')
plt.ylabel('Frequency')
plt.show()
# ancora una volta, la maggioranza dei casi ha una durata piccolissima
```

## Typical filter:

1. **Control-flow**
   1. Start/End activities
   2. Directly-follows
   3. Prefixes/Suffixes
   4. Case Size
2. **Time**
   1. TimeFrame
   2. Throughput
3. **Attibutes**:
   1. String attributes:
   2. Numeric Attributes